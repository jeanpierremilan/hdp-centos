 +++++++++++++ #
#   DEV MACHINE #
# +++++++++++++ #

#warm-up
sudo yum update -y
sudo yum install -y libaio ntp git wget 

#swap, hdds partitions
sudo -i
lsblk
fdsik /dev/sdX
#CREATE 50G SWAP
#CREATE THE PARTITIONS FOR HDP
mkswap -L SWAP /dev/sdX1
swapon /dev/sdX1
lsblk
free
mkfs.ext4 /dev/sdc2
mkdir /mnt/data
blkid
echo "UUID=XXXXXXXXXX /mnt/data ext4 defaults 0 0" | tee -a /etc/fstab
echo "UUID=YYYYYY none swap defaults 0 0" | tee -a /etc/fstab
mount -a
df -Th

# create raid1 in master node
mdadm --create /dev/md0 --level=1 --raid-devices=2 /dev/sdc2 /dev/sdd2
mkfs.ext4 /dev/md0
blkid 
echo "UUID=XXXXXXXXXX /mnt/data ext4 defaults 0 0" | tee -a /etc/fstab


#Check DNS and NSCD in all nodes
hostnamectl set-hostname dev.local
echo "10.0.0.5 dev.local" | tee -a /etc/hosts
echo "10.0.0.4 master.local" | tee -a /etc/hosts
echo "10.0.0.6 data1.local" | tee -a /etc/hosts
echo "10.0.0.7 data2.local" | tee -a /etc/hosts
echo "10.0.0.8 data3.local" | tee -a /etc/hosts

sed -i 's/localhost.localdomain/dev.local/g' /etc/sysconfig/network

for i in {dev.local,master.local,data1.local,data2.local,data3.local}; do ping -c 2 $i; done

#Enable NTP on the Cluster
systemctl enable ntpd
systemctl start ntpd
systemctl status ntpd

#SAP Vora Distributed Log (DLog)
cat /proc/sys/fs/file-max
echo "fs.file-max=983040" | tee -a /etc/sysctl.conf
sysctl -p
cat /etc/security/limits.conf
echo "*                -       nofile          1000000" | tee -a /etc/security/limits.conf
cat /etc/security/limits.conf
locale
export LC_ALL=en_US.UTF-8
locale
#Log out or reboot so that the ulimit change takes effect.
exit
sudo su -
echo umask 0022 | tee -a /etc/profile

#Configure iptables
systemctl disable firewalld
service firewalld stop

#Disable SELinux, PackageKit and Check umask Value
setenforce 0
sed -i 's/enforcing/disabled/g' /etc/selinux/config

reboot

#Set Up Service User Accounts
sudo -i
chmod 700 /etc/sudoers
echo "%sysadmin ALL=(ALL) NOPASSWD:ALL" | tee -a /etc/sudoers
chmod 440 /etc/sudoers
groupadd sysadmin
/usr/sbin/useradd -m -g users -G sysadmin hdp_admin
su - hdp_admin

#Set up Password-less SSH
ssh-keygen -t rsa
PRESS RETURN
PRESS RETURN

chmod 700 ~/
chmod 700 ~/.ssh
cat ~/.ssh/id_rsa.pub >~/.ssh/authorized_keys
chmod 600 ~/.ssh/authorized_keys
cat ~/.ssh/id_rsa.pub
ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC88ES9ZF881mgFiYgpApKVXeFnXa4SWWGdT+2yvyzxFt/tsoQocmCjixDvH2fSTRJpb3ToD5BK5pfnNsg9QjLYFW+msEY5b5ncomdmmCbPcOhbBQNDxd3L15gmqlC/Mko49Ma6hix6Kl553Gt1iNdHVnWDk4G2JA+6Fk0i0aCWlBesycSZcUuGQlxIUGW1y+CLYjSxnru6IBkJAWWT4OIytZfDz2fCi5YBrYO4n70XAsnxc2slvFLfVRkz3tQCyt/xNA6T8xNMSv+fgifPxdyH29T/Y4NndHLt1DNi9OkPuEFLJodEhzfgjyXeKmQPn0AGQbk8HhG350dQRt3EOdsL hdp_admin@hdpdev.io

# in data node as hdp_admin user
mkdir ~/.ssh
vi ~/.ssh/authorized_keys #copy paste the public key
chmod 600 ~/.ssh/authorized_keys


cat ~/.ssh/id_rsa
-----BEGIN RSA PRIVATE KEY-----
MIIEpQIBAAKCAQEAvPBEvWRfPNZoBYmIKQKSlV3hZ12uEllhnU/tsr8s8Rbf7bKE
KHJgo4sQ7x9n0k0SaW906A+QSuaX5zbIPUIy2BVvprBGOW+Z3KJnZpgmz3DoWwUD
Q8Xdy9eYJqpQvzJKOPTGuoYseipeedxrdYjXR1Z1g5OBtiQPuhZNItGglpQXrMnE
mXFLhkJcSFBltcvgi2I0sZ67uiAZCQFlk+DiMrWXw89nwouWAa2DuJ+9FwLJ8XNr
JbxS31UZM97UAsrf8TQOk/MTTEr/n4Inz8Xch9vU/2ODZ3Ry7dQzYvTpD7hBSyaH
RIc34I8l3ipkD59ABkG5PB4Rt+dHUEbdxDnbCwIDAQABAoIBAHqoONgDO1jRkIJX
wM9u3h6d7eEfDgzXfemUpCmwtsplIUa859MiPd6uP3Ydm1cJtfRA6VJmGDSn8uGl
zOUH/t4NPIioU06iRu7Xd+1AxaFSGWZNoMr0v4LAGgSv+O/HBCxprEIXLPS6YM8X
66NIDQpA9NVQg5rtWlBiff0mvexCZUOF0j9lFtMhGVp4t56UE77+i0z0GvW4p65I
7plNyKn3IyLOaZgoi0Aq5F74fYHuMUgW7s5qGJv+9SJhMv6eKtGqO7adoYs4R2At
4NgxqTwifH04lT9yL3/8FOvNpR56qbJfWG74G5qFr5wDv84pLOVlJJAvkjGJCNJp
qsV+XnECgYEA8QVNyybbZ4/PYDZExvMBqwC5xcBSOvg5d90Prk1htoyfkC/8d7j0
N1H2Bkv+KckdK2IuUj0cXat8P9gTAyhl8nJcGDNhEazYBRec7IOJ6coO+kSHOuuu
245lVrZUhRqZLhf/UUU1jZ6TapWPFhbFBxVESdpAUC3iuX5YNUxOxmUCgYEAyK5T
OoIgHLCXkLGCCNVGprDy0F+FqkagZw28Lud/b3leaF4VK6mkoorCZs0aUnA1MXID
jfe43LNf0/ioZDLr8ODIQwE9nFbcZKf+pC+dPBojl4uxUZ387hzyka3NbaTnYWQH
ddYD01N8LpuJe7bHmXNtOC+r5Y/3nYDzyhvNjK8CgYEAr8aieaqZEOo833o0prPL
X59j3OjqD7/Aib5bFU+yF6RvKoUUp2voEABiYS1C686Uy3MLPUb8TQQn1ZgMJZ1s
3Giy6aNvA6j+0pfqBkPYbPzD95nK1hnUqKMtHRRUh85iMM+nkOncqLAh6vWwKPfq
McQ8zW+yv8b3NzkBs703JlECgYEAq+8XJZiZFJuDr5c1loXLu0L9NeOxLFrIUKKm
6PSz7PjqFrcMolT81bnWpqbw91JlcJCDVR25R3VzE7QF6ZrrJHwhd67Xb1vIBYEd
HOQibXzjo5IugIPAUO2CMafbyiWvi3RdI+sRafwr9+6QUusdhW4sVCgbuVmoR0xV
EoLRj8cCgYEAyPWOPYjJkvZcqwt/tXj482sA6Oe5iEOA+iuyKygiPzH76nGtSMOX
dcqquemM9ZgnAHMEdMl80ehTml18rrWTk6yscK0n04ju0T2oD5eg7mc4xq3LWpXU
bjUq+voiTLlHqWPLBOTulDZ/eIpv7Gi9xfMn40Ing+x6R2OhC28cL08=
-----END RSA PRIVATE KEY-----



#Ambari, HDP, and HDF support umask values of 022
umask

# +++++++++++++ #
#   AMBARI      #
# +++++++++++++ #
su - hdp_admin

cd /etc/yum.repos.d

sudo wget http://public-repo-1.hortonworks.com/ambari/centos7/2.x/updates/2.5.2.0/ambari.repo -O /etc/yum.repos.d/ambari.repo
sudo yum repolist
sudo yum install -y ambari-server

sudo ambari-server setup
ACCEPT DEFAULTS

# +++++++++++++ #
#   HIVE CONIFG #
# +++++++++++++ #

#install MariaDB for HIVE
sudo yum install -y mariadb mariadb-server mysql-connector-java 
ls /usr/share/java/mysql-connector-java.jar

sudo ambari-server setup --jdbc-db=mysql --jdbc-driver=/usr/share/java/mysql-connector-java.jar

sudo systemctl enable mariadb
sudo systemctl start mariadb
mysql -u root -p
 
CREATE USER 'hive'@'%' IDENTIFIED BY 'jfkodf080980LLL87877!!!';
GRANT ALL PRIVILEGES ON *.* TO 'hive'@'%';
CREATE USER 'hive'@'localhost' IDENTIFIED BY 'jfkodf080980LLL87877!!!';
GRANT ALL PRIVILEGES ON *.* TO 'hive'@'localhost';
CREATE USER 'hive'@'dev.local' IDENTIFIED BY 'jfkodf080980LLL87877!!!';
GRANT ALL PRIVILEGES ON *.* TO 'hive'@'dev.local';
FLUSH PRIVILEGES;
exit;

mysql -u hive -p
CREATE DATABASE hive;
exit;

# +++++++++++++ #
#  OOZIE CONIFG #
# +++++++++++++ #

#install MariaDB for OOZIE
mysql -u root -p
 
CREATE USER 'oozie'@'%' IDENTIFIED BY 'jfkodf080980LLL87877!!!';
GRANT ALL PRIVILEGES ON *.* TO 'oozie'@'%';
CREATE USER 'oozie'@'localhost' IDENTIFIED BY 'jfkodf080980LLL87877!!!';
GRANT ALL PRIVILEGES ON *.* TO 'oozie'@'localhost';
CREATE USER 'oozie'@'dev.local' IDENTIFIED BY 'jfkodf080980LLL87877!!!';
GRANT ALL PRIVILEGES ON *.* TO 'oozie'@'dev.local';
FLUSH PRIVILEGES;
exit;

mysql -u oozie -p
CREATE DATABASE oozie;
exit;

# +++++++++++++ #
#   AMBARI      #
# +++++++++++++ #

sudo ambari-server start
#goto ambari web console 
#eg. http://gphdpdev.northeurope.cloudapp.azure.com:8080/#/login

-- Install HDP 2.6 with repo 2.6.2
-- Install HDP 2.6 with repo 2.6.2
-- Create Cluster; When selecting key, use the bottom Private one 
-- use "hdp_admin" user
-- Install HDFS, Yarn, Spark, Ambari Metrics, Hive (needed because of Spark), No Zookeeper
-- assign EVERYTHING on all slaves and clients
Admin Name : admin
Cluster Name : b2hdp
Total Hosts : 1 (1 new)
Repositories:
	redhat7 (HDP-2.6): 
		http://public-repo-1.hortonworks.com/HDP/centos7/2.x/updates/2.6.2.0
	redhat7 (HDP-UTILS-1.1.0.21): 
		http://public-repo-1.hortonworks.com/HDP-UTILS-1.1.0.21/repos/centos7
Services:
	HDFS
		DataNode : 1 host
		NameNode : dev.local
		NFSGateway : 0 host
		SNameNode : dev.local
	YARN + MapReduce2
		App Timeline Server : dev.local
		NodeManager : 1 host
		ResourceManager : dev.local
	Tez
		Clients : 1 host
	Hive
		Metastore : dev.local
		HiveServer2 : dev.local
		WebHCat Server : dev.local
		Database : Existing MySQL / MariaDB Database
	Pig
		Clients : 1 host
	Oozie
		Server : dev.local
		Database : Existing MySQL / MariaDB Database
	ZooKeeper
		Server : dev.local
	Ambari Infra
		Infra Solr Instance : dev.local
	Ambari Metrics
		Metrics Collector : dev.local
		Grafana : dev.local
	SmartSense
		Activity Analyzer : dev.local
		Activity Explorer : dev.local
		HST Server : dev.local
	Spark
		Livy Server : 0 host
		History Server : dev.local
		Thrift Server : 0 host	
	Spark2
		Livy for Spark2 Server : 0 host
		History Server : dev.local
		Thrift Server : 0 host
	Zeppelin Notebook
		Notebook : dev.local
	Slider
		Clients : 1 host

+++++++++++++++
+ AMBARI HDFS +
+++++++++++++++

The cluster consists of 1 hosts
Installed and started services successfully on 1 new host
	Master services installed
	NameNode installed on dev.local
	SNameNode installed on dev.local
	ResourceManager installed on dev.local
	History Server installed on dev.local
	HiveServer2 installed on dev.local
	Oozie Server installed on dev.local
All services started
All tests passed
Install and start completed in 64 minutes and 1 seconds
		
+++++++++++++++
+  TEST HDFS  +
+++++++++++++++

exit
su - hdfs
hdfs dfs -mkdir /user/admin
hdfs dfs -chown admin:hadoop /user/admin
echo "1,2,Hello" > test.csv
hdfs dfs -ls /user/admin
hdfs dfs -put test.csv /user/admin
hdfs dfs -ls /user/admin
hdfs dfs -cat /user/admin/test.csv
exit

++++++++++++++++++
+SETUP SPARK 1.6 +
++++++++++++++++++

sudo su -
sudo vi /etc/bash.bashrc
-- Type G, I (in VI)
Paste ;
## Paths in bash.bashrc FOR AMBARI ##
export JAVA_HOME=/usr/jdk64/jdk1.8.0_112
export HADOOP_CONF_DIR=/etc/hadoop/conf
export SPARK_HOME=/usr/hdp/2.6.2.0-205/spark/
export SPARK_CONF_DIR=$SPARK_HOME/conf
#export SPARK_MAJOR_VERSION=2
export PATH=$PATH:$SPARK_HOME/bin

exit
sudo su -
su - cluster_admin

++++++++++++++++
+TEST SPARK 1.6+
++++++++++++++++

spark-submit --version

# YOU SHOULD SEE THIS
Multiple versions of Spark are installed but SPARK_MAJOR_VERSION is not set
Spark1 will be picked by default
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 1.6.3
      /_/

Type --help for more information.

# LUNCH EXAMPLE
spark-submit --class org.apache.spark.examples.SparkPi --queue default $SPARK_HOME/examples/jars/spark-examples*.jar 10

# should see "Pi is roughly 3.140967140967141"	

++++++++++++++++
+ SETUP SPARK 2+
++++++++++++++++

sudo su -
sudo vi /etc/bash.bashrc
-- Type G, I (in VI)
Paste ;
## Paths in bash.bashrc FOR AMBARI ##
export JAVA_HOME=/usr/jdk64/jdk1.8.0_112
export HADOOP_CONF_DIR=/etc/hadoop/conf
export SPARK_HOME=/usr/hdp/2.6.2.0-205/spark2/
export SPARK_CONF_DIR=$SPARK_HOME/conf
export SPARK_MAJOR_VERSION=2
export PATH=$PATH:$SPARK_HOME/bin

exit
sudo su -
su - cluster_admin

++++++++++++++++
+  TEST SPARK 2+
++++++++++++++++

spark-submit --version

# YOU SHOULD SEE THIS
SPARK_MAJOR_VERSION is set to 2, using Spark2
Welcome to
      ____              __
     / __/__  ___ _____/ /__
    _\ \/ _ \/ _ `/ __/  '_/
   /___/ .__/\_,_/_/ /_/\_\   version 2.1.1.2.6.2.0-205
      /_/

Using Scala version 2.11.8, Java HotSpot(TM) 64-Bit Server VM, 1.8.0_112
Branch HEAD
Compiled by user jenkins on 2017-08-26T09:32:23Z
Revision a2efc34efde0fd268a9f83ea1861bd2548a8c188
Url git@github.com:hortonworks/spark2.git
Type --help for more information.

# LUNCH EXAMPLE
spark-submit --class org.apache.spark.examples.SparkPi --num-executors 2 --driver-memory 512m --executor-memory 512m --executor-cores 2 --queue default $SPARK_HOME/examples/jars/spark-examples*.jar 10

# should see "Pi is roughly 3.143307143307143"		

++++++++++++++++
+ HUE INSTALL +
++++++++++++++++
https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.2/bk_command-line-installation/content/configure_hdp_support_hue.html
http://hadooppowered.com/2014/05/25/install-or-remove-services-in-HUE/
#Stop All Ambari services
sudo yum install -y hue

#Using Hue with MySQL
mysql -u root -p
CREATE USER hue IDENTIFIED BY 'password';
GRANT ALL PRIVILEGES on *.* to 'hue'@'localhost' WITH GRANT OPTION;
GRANT ALL on hue.* to 'hue'@'localhost' IDENTIFIED BY 'password';
GRANT ALL PRIVILEGES on *.* to 'hue'@'dev.local' WITH GRANT OPTION;
GRANT ALL on hue.* to 'hue'@'dev.local' IDENTIFIED BY 'password';
GRANT ALL PRIVILEGES on *.* to 'hue'@'%' WITH GRANT OPTION;
GRANT ALL on hue.* to 'hue'@'%' IDENTIFIED BY 'password';
FLUSH PRIVILEGES;
exit

mysql -u hue -p
CREATE DATABASE hue;
SHOW DATABASES;
exit

cd /usr/lib/hue
source build/env/bin/activate
hue syncdb --noinput
hue migrate
deactivate

#check the installation
/etc/init.d/hue start
/etc/init.d/hue status
/etc/init.d/hue stop

++++++++++++++++
+ Test WebHDFS +
++++++++++++++++

curl -i "http://dev.local:50070/webhdfs/v1/user/admin/?op=LISTSTATUS"

++++++++++++++++
+ SolR INSTALL +
++++++++++++++++

# https://docs.hortonworks.com/HDPDocuments/HDP2/HDP-2.6.0/bk_solr-search-installation/content/ch_hdp-search-install-ambari.html
cd /tmp
wget http://public-repo-1.hortonworks.com/HDP-SOLR/hdp-solr-ambari-mp/solr-service-mpack-2.2.8.tar.gz
sudo -i 
ambari-server install-mpack --mpack=/tmp/solr-service-mpack-2.2.8.tar.gz
yum install lucidworks-hdpsearch
# https://doc.lucidworks.com/lucidworks-hdpsearch/2.6/Guide-Install-Ambari.html#_startup-option-reference

Solr
	Solr : dev.local

# in Ambari restart all services

sudo -u hdfs hdfs dfsadmin -safemode leave

++++++++++++++++
+ NiFI Setup   +
++++++++++++++++

sudo -i
cd /tmp
wget http://mirror.easyname.ch/apache/nifi/1.3.0/nifi-1.3.0-bin.tar.gz
tar -xzvf nifi-1.3.0-bin.tar.gz

mv ./nifi-1.3.0 /etc/
cd /etc/nifi-1.3.0/bin
./nifi.sh install

cd ../conf
# change 
# nifi.web.http.host=0.0.0.0
# nifi.web.http.port=8282

vi nifi.properties

systemctl enable nifi
systemctl start nifi
